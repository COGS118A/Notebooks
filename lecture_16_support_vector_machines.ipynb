{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow along and play with this notebook by clicking the badge below\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jasongfleischer/COGS118A/blob/main/demo_notebooks/lecture_16_support_vector_machines.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a classification task, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
    "For two dimensional data like that shown here, this is a task we could do by hand.\n",
    "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "\n",
    "\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
    "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines offer one way to improve on this.\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
    "Support vector machines are an example of such a *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a support vector machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
    "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=3, facecolors='none', edgecolor='k');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=ax)\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [30, 60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for just 10 training points, in the middle we see  60 training points and on the right panel 120 points.  While going from 30 to 60 caused changes, going from 60 to 120 did not becuase the support vectors (the points on the margin line) had not changed.\n",
    "\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond linear boundaries: Kernel SVM\n",
    "\n",
    "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
    "We have seen a version of kernels before: Remember polynomial features from the regression lectures?  \n",
    "\n",
    "There we projected our data into higher-dimensional space defined by polynomials functions, and thereby were able to fit for nonlinear relationships with a linear regression.\n",
    "\n",
    "In SVM models, we can use a version of the same idea.\n",
    "To motivate the need for kernels, let's look at some data that is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2');\n",
    "#plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
    "\n",
    "One simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding r as a 3rd dimension gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette()[:2])\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], phi, c=y, s=7, cmap=cmap)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('$\\phi(\\mathbf{x})_3$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *phi*=0.7.\n",
    "\n",
    "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none', edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap?\n",
    "For example, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
    "For very large $C$, the margin is hard, and points cannot lie in it.\n",
    "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=axi)\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none', edgecolor='k');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar model selection procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this will take around 2-3 minutes to run\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# lets get some 🐧 data to work with\n",
    "df = sns.load_dataset('penguins').dropna()\n",
    "\n",
    "# what we want to predict\n",
    "y = df['species']\n",
    "X = df.drop(columns=['island','species'])\n",
    "\n",
    "# setup a way to transform the categorical variables into one hot, \n",
    "# and to z-score the numeric variables\n",
    "# remember how we had to do this with a bunch of different\n",
    "# steps manually the last time around??\n",
    "# if you'd like to understand how to do this more generally\n",
    "# search the sklearn docs for ColumnTransformer!\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# SVM have very different hyperparameters depending on which kernel\n",
    "# potential hyperparams are gamma, coef0, degree and kernel.\n",
    "# here <x,x'> and ||x-x'|| are similarities between two vectors x,x'\n",
    "# linear kernel:     <x,x'>  [no hyperparams]\n",
    "# poly kernel:       ( gamma * <x,x'> + coef0 )^degree \n",
    "# rbf kernel:        exp( - gamma * ||x-x'||^2 )\n",
    "# sigmoid kernel:    tanh( gamma * <x,x'> + coef0 )\n",
    "\n",
    "# so how can we search using only using hypers appropriate for each kernel?\n",
    "# like this...make a list of different search spaces you want to execute\n",
    "search_space = [{'classifier__kernel': ['linear'],\n",
    "                 'classifier__C': np.logspace(-3, 2, 11) #11 steps between 10^-3 to 10^2\n",
    "                },\n",
    "                {'classifier__kernel': ['poly'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__degree': range(2,14),\n",
    "                 # NOPE... this isnt often important 'classifier__coef0': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                {'classifier__kernel': ['rbf'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=1)\n",
    "# play with different verbose=??\n",
    "\n",
    "# now lets split off some of our data for testing set... the rest will be used from cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, random_state=101)\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "%time best_model.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# lets find out how well we do on the test set...\n",
    "print(best_model.best_params_)\n",
    "yhat = best_model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run EXACTLY the same grid search as before, but this time\n",
    "# lets use a different kind of dataset, a tougher problem, so \n",
    "# maybe the best classifier wont be the simple linear hard classifier as\n",
    "# with penguins\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# lets try some very differen data\n",
    "# these are measurements of various tumors... the job is to \n",
    "# classifiy the tumor as malignant or benign\n",
    "# data from https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "cdata = load_breast_cancer(as_frame=True)\n",
    "X = cdata['data']\n",
    "y = cdata['target']\n",
    "\n",
    "# setup a way to transform the categorical variables into one hot, \n",
    "# and to z-score the numeric variables\n",
    "# remember how we had to do this with a bunch of different\n",
    "# steps manually the last time around??\n",
    "# if you'd like to understand how to do this more generally\n",
    "# search the sklearn docs for ColumnTransformer!\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "# NOTE: without the max_iter argument my computer was starting \n",
    "# to melt after 20 min... this data is so much harder to deal with\n",
    "# and especially high C - hard margin SVM are still NOT CONVERGING after\n",
    "# very long periods of time\n",
    "# you can see the problem in the thousands of convergence warnings below\n",
    "# once the max_iter is reached\n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', SVC(max_iter=5000))])\n",
    "\n",
    "# SVM have very different hyperparameters depending on which kernel\n",
    "# potential hyperparams are gamma, coef0, degree and kernel.\n",
    "# linear kernel:     <x,x'>  [no hyperparams]\n",
    "# poly kernel:       ( gamma * <x,x'> + coef0 )^degree \n",
    "# rbf kernel:        exp( - gamma * ||x-x'||^2 )\n",
    "# sigmoid kernel:    tanh( gamma * <x,x'> + coef0 )\n",
    "\n",
    "# so how can we search all the hypers, only using hypers appropriate for each kernel?\n",
    "# like this...make a list of different search spaces you want to execute\n",
    "search_space = [{'classifier__kernel': ['linear'],\n",
    "                 'classifier__C': np.logspace(-3, 2, 11) #11 steps between 10^-3 to 10^2\n",
    "                },\n",
    "                {'classifier__kernel': ['poly'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__degree': range(2,14),\n",
    "                 # NOPE... this isnt often important 'classifier__coef0': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                {'classifier__kernel': ['rbf'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=1)\n",
    "# play with different verbose=??\n",
    "\n",
    "# now lets split off some of our data for testing set... the rest will be used from cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, random_state=101)\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "%time best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# lets find out how well we do on the test set...\n",
    "print(best_model.best_params_)\n",
    "yhat = best_model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Summary\n",
    "\n",
    "We have seen here a brief intuitive introduction to the principals behind support vector machines.\n",
    "These methods are a powerful classification method for a number of reasons:\n",
    "\n",
    "- Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\n",
    "- Once the model is trained, the prediction phase is very fast.\n",
    "- Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\n",
    "- Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
    "\n",
    "However, SVMs have several disadvantages as well:\n",
    "\n",
    "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\n",
    "- The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n",
    "- The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the ``probability`` parameter of ``SVC``), but this extra estimation is costly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook contains excerpts of text and code from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook). The text from the book was released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code from the book is released under the [MIT license](https://opensource.org/licenses/MIT).* \n",
    "\n",
    "*This notebook also contains text and code written by Jason Fleischer.  I'm too tired to think much about it, so I'm happy to release my work under the same licenses*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
