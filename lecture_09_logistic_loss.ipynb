{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71f71e0",
   "metadata": {},
   "source": [
    "You can follow along and play with this notebook by clicking the badge below\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS118A/demo_notebooks/blob/main/lecture_09_logistic_loss.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need this for live 3d rotation, i did\n",
    "%pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1291ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two useful data viz libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setup plotting in a notebook in a reasonable way\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# default figure aesthetics I'll be using, \n",
    "# there are other choices, see seaborn docs\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721df741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get some ðŸ§ data to work with\n",
    "penguins = sns.load_dataset(\"penguins\").dropna()\n",
    "\n",
    "# make a combo category\n",
    "penguins['species_x_sex'] = penguins['species']+', '+penguins['sex']\n",
    "\n",
    "# this makes color sequence light/dark of the same color\n",
    "# to encode sex for a given species, see seaborn docs\n",
    "with sns.color_palette(\"Paired\"):\n",
    "\n",
    "    # pariplot shows all pairs of real valued variables\n",
    "    sns.pairplot(penguins, hue='species_x_sex', \n",
    "                 hue_order=[ # forces the right order of colors \n",
    "                    'Adelie, Male', 'Adelie, Female',\n",
    "                    'Chinstrap, Male', 'Chinstrap, Female',\n",
    "                    'Gentoo, Male', 'Gentoo, Female']\n",
    "                );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53d947",
   "metadata": {},
   "source": [
    "Look at flipper length above.  It seems like it's probably the best single variable that could seperate the big birds (Gentoo) from the small ones.  I want an ultra simple model to look at some ideas with Logistic Regression. So it's fortunate that this is just distinguishing Gentoo vs the combo of Adelie/Chinstrap.  After all, at its simplest form LR is a BINARY classifier.\n",
    "\n",
    "\n",
    "\n",
    "So we're going to do Logistic Regression using flipper length of the birds as input, where Gentoo are positive class, and negative class is any bird not a Gentoo (Adelie, Chinstrap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962131d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# here's our raw data, this time include sex!\n",
    "X = penguins[['flipper_length_mm']]\n",
    "y = penguins['species'].map( \n",
    "    {'Gentoo':'Gentoo', 'Adelie':'Other', 'Chinstrap':'Other'}\n",
    ")\n",
    "\n",
    "# create a training and test set with test_size = 0.5 and random_state = 101\n",
    "raise NotImplementedError\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5648471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# hey what's this penalty thing mean?? better look at the docs!\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "model = LogisticRegression(penalty='none')\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# how good will we do on training set error and test set error?\n",
    "yhat_train = raise NotImplementedError\n",
    "yhat_test = raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a379c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# accuracy scores for training and test sets\n",
    "acc_train = raise NotImplementedError\n",
    "acc_test = raise NotImplementedError\n",
    "\n",
    "print(f'training set accuracy (n={y_train.shape[0]}): {acc_train:4.3f}')\n",
    "print(f'test set accuracy (n={y_test.shape[0]}): {acc_test:4.3f}')\n",
    "print()\n",
    "print('classificiation report on test set performance')\n",
    "print(classification_report(y_test, yhat_test))\n",
    "print()\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, yhat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets discuss ROC curves now... \n",
    "# https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "# fundamentally, imagine moving the threshold on LR prediction (f(x) = 1 if logit(x)>threshold else -1) \n",
    "# back and forth...  BTW threshold is normally 0.5 \n",
    "# move towards the positive class.... it will catch more false negatives, and fewer false positives\n",
    "# towards the negative class... it will catch more false positives, and fewer false negatives\n",
    "# this what the ROC curve shows us... the inherent tradeoff between two kinds of errors\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.preprocessing import LabelBinarizer # some of these helper functions can't handle string labels, need +1/-1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(LabelBinarizer().fit_transform(y_test), \n",
    "                                 LabelBinarizer().fit_transform(yhat_test) )\n",
    "roc_auc = auc(fpr, tpr)\n",
    "display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "                                   estimator_name='example estimator')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the optimal parameters of logistic regression model\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc665cb2",
   "metadata": {},
   "source": [
    "Let's visualize the loss surface of that logistic regression.  We will take a small grid around the optimal values here and show the loss as a 3d height given the 2d coef + intercept values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss # this is the logisitc loss function\n",
    "import numpy as np\n",
    "\n",
    "dummy_model = LogisticRegression().fit(X_train,y_train)\n",
    "\n",
    "def loss(coef, intercept):\n",
    "    dummy_model.coef_ = np.array(coef).reshape((1,1))\n",
    "    dummy_model.intercept_ = np.array(intercept)\n",
    "    yhat_train = dummy_model.predict_proba(X_train)\n",
    "    return log_loss(y_train, yhat_train)\n",
    "\n",
    "vec_loss = np.vectorize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Make data.\n",
    "w = np.arange(-1,1,0.05)  # weight coef range around optimal\n",
    "b = np.arange(122, 142, 1)  # bias intercept range around optimal\n",
    "\n",
    "X, Y = np.meshgrid(w, b)\n",
    "Z = vec_loss(X, Y)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02782177",
   "metadata": {},
   "source": [
    "Seems very much like coef is much more important at least in this local area of the params.  Finding the absolute lowest point in that valley might be very hard... the gradients can be vanishingly small so movement towards an optimum requires a fair number of iterations.... or if your solver isn't up to the task it might stop short of optimal\n",
    "\n",
    "This problem is so simple it hardly will change anything, but in more extreme versions of this it can very much matter what your solver is and its settings as you struggle to follow a weak gradient... like this animation shows on a loss surface kinda like the one above\n",
    "\n",
    "\n",
    "![weak gradient causes slow gradient descent for many solvers](http://2.bp.blogspot.com/-L98w-SBmF58/VPmICIjKEKI/AAAAAAAACCs/rrFz3VetYmM/s1600/Beale%26amp%3B%23039%3Bs%2Bfunction%2B-%2BImgur.gif![image.png](attachment:image.png))\n",
    "\n",
    "That's an interesting idea, yeah?  Now go take a look again at the docs for LR https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression and look at the stuff about solver choices.  \n",
    "\n",
    "\n",
    "Some places to explore:\n",
    "\n",
    "1. Go back to the start and pick different solvers, see if the choice of solver will change your answers at all\n",
    "    1. If a solver doesn't work, can you find a set of solver parameters that will fix it?\n",
    "    1. Are there certain solvers that can't deal with certain penaltys?\n",
    "1. Try to do all the variables instead of just flipper length. How does that change the answer?\n",
    "    1. Use scaler() for reals. Like kNN, LR uses a Euclidean distance in the vector space and high magnitude variables will swamp smaller ones in that space.\n",
    "    1. Use onehot() if you encode sex... you know why\n",
    "1. Can you use LR to predict all 3 species?  How can that work, when LR is actually at its root a binary classifier????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c3513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
